{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60486f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification,\n",
    "                          Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('dataset_2_2.xlsx', sheet_name='Sheet1')\n",
    "df[['text', 'emotion']] = df['row1'].str.split(';', expand=True)\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "emotion_map = {'joy': 0, 'sadness': 1, 'anger': 2, 'fear': 3, 'love': 4, 'surprise': 5}\n",
    "df['label'] = df['emotion'].map(emotion_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c197dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc033b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = TweetDataset(X_train.values, y_train.values, tokenizer)\n",
    "test_dataset = TweetDataset(X_test.values, y_test.values, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e06e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=len(emotion_map)\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {'f1': f1_score(labels, preds, average='macro')}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = y_test.values\n",
    "print(\"Macro F1:\", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=emotion_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ee3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "print(\"Baseline Macro F1:\", f1_score(y_true, y_pred_lr, average='macro'))\n",
    "print(\"Baseline Report:\\n\", classification_report(y_true, y_pred_lr, target_names=emotion_map.keys()))\n",
    "\n",
    "# Save models\n",
    "model.save_pretrained('./emotion_bert_model')\n",
    "tokenizer.save_pretrained('./emotion_bert_model')\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f: pickle.dump(tfidf, f)\n",
    "with open('lr_model.pkl', 'wb') as f: pickle.dump(lr_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Tweet Emotion Classifier\")\n",
    "st.write(\"Enter a tweet to predict its emotion using fine-tuned BERT and compare with Logistic Regression baseline.\")\n",
    "\n",
    "user_input = st.text_area(\"Enter your tweet:\")\n",
    "if user_input:\n",
    "    cleaned_input = preprocess_text(user_input)\n",
    "    encoding = tokenizer(cleaned_input, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        emotion = {v: k for k, v in emotion_map.items()}[pred_label]\n",
    "        st.write(f\"**Predicted Emotion (BERT):** {emotion}\")\n",
    "        st.write(\"**Probabilities (BERT):**\")\n",
    "        for emo, prob in zip(emotion_map.keys(), probs[0].numpy()):\n",
    "            st.write(f\"{emo}: {prob:.4f}\")\n",
    "    # Logistic Regression\n",
    "    input_tfidf = tfidf.transform([cleaned_input])\n",
    "    lr_pred = lr_model.predict(input_tfidf)[0]\n",
    "    lr_emotion = {v: k for k, v in emotion_map.items()}[lr_pred]\n",
    "    lr_probs = lr_model.predict_proba(input_tfidf)[0]\n",
    "    st.write(f\"**Predicted Emotion (TF-IDF + Logistic Regression):** {lr_emotion}\")\n",
    "    st.write(\"**Probabilities (Logistic Regression):**\")\n",
    "    for emo, prob in zip(emotion_map.keys(), lr_probs):\n",
    "        st.write(f\"{emo}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
